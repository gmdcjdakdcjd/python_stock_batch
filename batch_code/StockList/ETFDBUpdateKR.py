import os

import pandas as pd
from bs4 import BeautifulSoup
import requests
import json
from datetime import datetime
from pymongo import MongoClient

from common.mongo_util import MongoDB


class DBUpdater:
    def __init__(self):
        """ìƒì„±ì: MongoDB ì—°ê²°"""

        # ê¸°ì¡´ MariaDB ì½”ë“œ (ì‚¬ìš© ì•ˆ í•¨)
        # self.conn = pymysql.connect(...)
        # self.conn.commit()

        mongo = MongoDB()
        self.mongo = mongo  # ğŸ”¥ ì¶”ê°€í•´ì•¼ í•¨
        self.col_etf_info = mongo.db["etf_info_kr"]
        self.col_etf_daily = mongo.db["etf_daily_price_kr"]

        self.codes = dict()  # {'ì½”ë“œ': 'ì´ë¦„'}

    # -------------------------------------------------
    # ë„¤ì´ë²„ì—ì„œ ETF ì¼ë³„ ì‹œì„¸ ì½ê¸°
    # -------------------------------------------------
    def read_naver(self, code, company, pages_to_fetch):
        try:
            url = f"http://finance.naver.com/item/sise_day.nhn?code={code}"
            html = BeautifulSoup(
                requests.get(url, headers={'User-agent': 'Mozilla/5.0'}).text, "lxml"
            )

            # í˜ì´ì§€ ê³„ì‚°
            pgrr = html.find("td", class_="pgRR")
            if pgrr is None:
                lastpage = 1
            else:
                lastpage = str(pgrr.a['href']).split('=')[-1]

            df = pd.DataFrame()
            pages = min(int(lastpage), pages_to_fetch)

            for page in range(1, pages + 1):
                pg_url = f"{url}&page={page}"
                page_df = pd.read_html(
                    requests.get(pg_url, headers={'User-agent': 'Mozilla/5.0'}).text
                )[0]

                df = pd.concat([df, page_df], ignore_index=True)

                print(f"[INFO] {company}({code}) {page}/{pages} í˜ì´ì§€ ìˆ˜ì§‘ ì¤‘...", end="\r")

            # ì»¬ëŸ¼ ì •ë¦¬
            df = df.rename(columns={
                'ë‚ ì§œ': 'date',
                'ì¢…ê°€': 'close',
                'ì „ì¼ë¹„': 'diff',
                'ì‹œê°€': 'open',
                'ê³ ê°€': 'high',
                'ì €ê°€': 'low',
                'ê±°ë˜ëŸ‰': 'volume'
            })

            # ë‚ ì§œ ì •ê·œí™”
            df['date'] = df['date'].astype(str).str.replace('.', '-', regex=False)
            df['date'] = pd.to_datetime(df['date'], errors='coerce').dt.strftime('%Y-%m-%d')
            df = df.dropna(subset=['date'])

            # ìˆ«ì ë³€í™˜
            df['diff'] = df['diff'].astype(str).str.extract(r'(\d+)')
            df = df.dropna()

            df[['close', 'diff', 'open', 'high', 'low', 'volume']] = df[
                ['close', 'diff', 'open', 'high', 'low', 'volume']
            ].astype(int)

            return df[['date', 'open', 'high', 'low', 'close', 'diff', 'volume']]

        except Exception as e:
            print(f"[ERROR] {company}({code}) ìˆ˜ì§‘ ì‹¤íŒ¨ â†’ {e}")
            return None

    # -------------------------------------------------
    # MongoDB ì €ì¥ (REPLACE INTO â†’ upsert)
    # -------------------------------------------------
    def replace_into_db(self, df, num, code, company):
        for r in df.itertuples():
            dt = pd.to_datetime(r.date).to_pydatetime()  # datetime ë³€í™˜

            doc = {
                "code": code,
                "date": dt,  # datetime
                "open": r.open,
                "high": r.high,
                "low": r.low,
                "close": r.close,
                "diff": r.diff,
                "volume": r.volume,
                "last_update": datetime.now()  # â˜… datetime
            }

            self.col_etf_daily.update_one(
                {"code": code, "date": dt},  # ì¡°ê±´ë„ datetime
                {"$set": doc},
                upsert=True
            )

        print(f"[OK] #{num + 1:04d} {company}({code}) {len(df)} rows ì €ì¥")
        print(f"ROWCOUNT={len(df)}")
        return len(df)

    # -------------------------------------------------
    # ETF ì½”ë“œ ë¡œë“œ (MongoDBì—ì„œ)
    # -------------------------------------------------
    def load_codes_from_db(self):
        """KODEX ETFë§Œ ë¡œë“œ"""
        cursor = self.col_etf_info.find(
            {"manager": "ì‚¼ì„±ìì‚°ìš´ìš©"},  # ìš´ìš©ì‚¬ ê¸°ì¤€
            {"code": 1, "name": 1}
        )

        self.codes = {doc["code"]: doc["name"] for doc in cursor}
        print(f"[INFO] {len(self.codes)}ê°œ ETF ë¡œë“œ ì™„ë£Œ (ì‚¼ì„±ìì‚°ìš´ìš©)")

    # -------------------------------------------------
    # ì „ì²´ ì—…ë°ì´íŠ¸
    # -------------------------------------------------
    def update_daily_price(self, pages_to_fetch):
        total_count = 0
        processed = 0

        for idx, code in enumerate(self.codes):
            df = self.read_naver(code, self.codes[code], pages_to_fetch)
            if df is None or df.empty:
                continue
            total_count += self.replace_into_db(df, idx, code, self.codes[code])
            processed += 1

        print(f"ROWCOUNT={total_count}")
        print(f"CODECOUNT={processed}")

    def execute_daily(self):
        self.load_codes_from_db()

        # í˜„ì¬ íŒŒì¼ ê²½ë¡œ ê¸°ë°˜ ì ˆëŒ€ ê²½ë¡œ ë§Œë“¤ê¸°
        BASE_DIR = os.path.dirname(os.path.abspath(__file__))
        CONFIG_PATH = os.path.join(BASE_DIR, "config.json")

        try:
            with open(CONFIG_PATH, "r", encoding="utf-8") as f:
                pages_to_fetch = json.load(f).get("pages_to_fetch", 1)
        except FileNotFoundError:
            pages_to_fetch = 1
            with open(CONFIG_PATH, "w", encoding="utf-8") as f:
                json.dump({"pages_to_fetch": 1}, f, indent=4, ensure_ascii=False)

        self.update_daily_price(pages_to_fetch)


if __name__ == '__main__':
    dbu = DBUpdater()
    dbu.execute_daily()
    dbu.mongo.close()   # ëª…ì‹œì  ì¢…ë£Œ