import pandas as pd
from bs4 import BeautifulSoup
import pymysql, calendar, time, json
import requests
from datetime import datetime
from threading import Timer


class DBUpdater:
    def __init__(self):
        """ìƒì„±ì: MariaDB ì—°ê²° ë° ì¢…ëª©ì½”ë“œ ë”•ì…”ë„ˆë¦¬ ìƒì„±"""
        self.conn = pymysql.connect(host='localhost', user='root',
                                    password='0806', db='INVESTAR', charset='utf8')


        self.conn.commit()
        self.codes = dict()

    def __del__(self):
        """ì†Œë©¸ì: MariaDB ì—°ê²° í•´ì œ"""
        self.conn.close()


    def read_naver(self, code, company, pages_to_fetch):
        """ë„¤ì´ë²„ì—ì„œ ì£¼ì‹ ì‹œì„¸ë¥¼ ì½ì–´ì„œ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë°˜í™˜"""
        try:
            url = f"http://finance.naver.com/item/sise_day.nhn?code={code}"
            html = BeautifulSoup(requests.get(url,
                                              headers={'User-agent': 'Mozilla/5.0'}).text, "lxml")
            pgrr = html.find("td", class_="pgRR")
            if pgrr is None:
                return None
            s = str(pgrr.a["href"]).split('=')
            lastpage = s[-1]
            df = pd.DataFrame()
            pages = min(int(lastpage), pages_to_fetch)
            for page in range(1, pages + 1):
                pg_url = '{}&page={}'.format(url, page)
                df = df.append(pd.read_html(requests.get(pg_url,
                                                         headers={'User-agent': 'Mozilla/5.0'}).text)[0])
                tmnow = datetime.now().strftime('%Y-%m-%d %H:%M')
                print('[{}] {} ({}) : {:04d}/{:04d} pages are downloading...'.
                      format(tmnow, company, code, page, pages), end="\r")
            df = df.rename(columns={'ë‚ ì§œ': 'date', 'ì¢…ê°€': 'close', 'ì „ì¼ë¹„': 'diff'
                , 'ì‹œê°€': 'open', 'ê³ ê°€': 'high', 'ì €ê°€': 'low', 'ê±°ë˜ëŸ‰': 'volume'})
            df['date'] = df['date'].replace('.', '-')
            df['diff'] = df['diff'].str.extract(r'(\d+)')
            df = df.dropna()
            df[['close', 'diff', 'open', 'high', 'low', 'volume']] = df[['close',
                                                                         'diff', 'open', 'high', 'low',
                                                                         'volume']].astype(int)
            df = df[['date', 'open', 'high', 'low', 'close', 'diff', 'volume']]
        except Exception as e:
            print('Exception occured :', str(e))
            return None
        return df

    def replace_into_db(self, df, num, code, company):
        """ë„¤ì´ë²„ì—ì„œ ì½ì–´ì˜¨ ì£¼ì‹ ì‹œì„¸ë¥¼ DBì— REPLACE"""
        with self.conn.cursor() as curs:
            for r in df.itertuples():
                sql = f"REPLACE INTO etf_daily_price VALUES ('{code}', " \
                      f"'{r.date}', {r.open}, {r.high}, {r.low}, {r.close}, " \
                      f"{r.diff}, {r.volume})"
                curs.execute(sql)
            self.conn.commit()
            print('[{}] #{:04d} {} ({}) : {} rows > REPLACE INTO etf_daily_price [OK]'
                  .format(datetime.now().strftime('%Y-%m-%d %H:%M'),
                          num + 1, company, code, len(df)))
            # âœ… ìë°”ì—ì„œ íŒŒì‹±í•  ìˆ˜ ìˆëŠ” í˜•ì‹
            print(f"ROWCOUNT={len(df)}")
            return len(df)  # ğŸ‘ˆ row count ë°˜í™˜

    def update_daily_price(self, pages_to_fetch):
        """KRX ìƒì¥ë²•ì¸ì˜ ì£¼ì‹ ì‹œì„¸ë¥¼ ë„¤ì´ë²„ë¡œë¶€í„° ì½ì–´ì„œ DBì— ì—…ë°ì´íŠ¸"""
        total_count = 0
        processed_codes = 0  # ğŸ‘ˆ ì¢…ëª© ê°œìˆ˜ ì¹´ìš´íŠ¸ìš©

        for idx, code in enumerate(self.codes):
            df = self.read_naver(code, self.codes[code], pages_to_fetch)
            if df is None:
                continue
            total_count += self.replace_into_db(df, idx, code, self.codes[code])
            processed_codes += 1  # ğŸ‘ˆ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ëœ ì¢…ëª©ë§Œ ì¹´ìš´íŠ¸

        #  ìë°”ì—ì„œ íŒŒì‹±í•  ìˆ˜ ìˆëŠ” í¬ë§·ìœ¼ë¡œ ì¶œë ¥
        print(f"ROWCOUNT={total_count}")
        print(f"CODECOUNT={processed_codes}")

    def load_codes_from_db(self):
        with self.conn.cursor() as curs:
            sql = "SELECT code,name FROM etf_info"
            curs.execute(sql)
            rows = curs.fetchall()
            self.codes = {code: name for code, name in rows}

    def execute_daily(self):
        """ì‹¤í–‰ ì¦‰ì‹œ ë° ë§¤ì¼ ì˜¤í›„ ë‹¤ì„¯ì‹œì— daily_price í…Œì´ë¸” ì—…ë°ì´íŠ¸"""
        self.load_codes_from_db()

        try:
            with open('config.json', 'r') as in_file:
                config = json.load(in_file)
                pages_to_fetch = config['pages_to_fetch']
        except FileNotFoundError:
            with open('config.json', 'w') as out_file:
                pages_to_fetch = 100
                config = {'pages_to_fetch': 1}
                json.dump(config, out_file)
        self.update_daily_price(pages_to_fetch)




if __name__ == '__main__':
    dbu = DBUpdater()
    dbu.execute_daily()
